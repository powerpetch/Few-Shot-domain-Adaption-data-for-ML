# ğŸ“‘ Project Index & File Guide

## Quick Navigation

### ğŸ¯ Getting Started
- **New to this project?** â†’ Start here: [QUICK_START.md](QUICK_START.md)
- **Want full details?** â†’ Read: [PROJECT_GUIDE.md](PROJECT_GUIDE.md)
- **Need overview?** â†’ See: [README.md](README.md)

---

## ğŸ“‚ Complete File Structure

### Root Level Documentation
```
d:\user\CEIPP\
â”‚
â”œâ”€â”€ README.md â­
â”‚   â””â”€ Project overview, quick links, learning paths
â”‚
â”œâ”€â”€ PROJECT_GUIDE.md â­â­â­
â”‚   â””â”€ Complete methodology, phase explanations, workflow
â”‚
â”œâ”€â”€ QUICK_START.md â­â­
â”‚   â””â”€ 3-step setup, troubleshooting, configuration
â”‚
â”œâ”€â”€ requirements.txt
â”‚   â””â”€ Python package dependencies (install with: pip install -r requirements.txt)
â”‚
â””â”€â”€ FILE_GUIDE.md (this file)
    â””â”€ Navigation and index of all files
```

### Python Scripts & Tools (`LLM/`)
```
LLM/
â”‚
â”œâ”€â”€ captioning_pipeline.py â­â­
â”‚   Purpose: Command-line batch processing of images
â”‚   Usage: python captioning_pipeline.py --input_dir ... --output_dir ...
â”‚   Best for: Large-scale automated caption generation
â”‚
â”œâ”€â”€ captioning_interactive.ipynb â­â­â­
â”‚   Purpose: Step-by-step interactive workflow
â”‚   Usage: jupyter notebook captioning_interactive.ipynb
â”‚   Best for: Learning, testing, visual feedback
â”‚   Includes: Dataset exploration, caption generation, visualization
â”‚
â”œâ”€â”€ data_explorer.py
â”‚   Purpose: Analyze dataset structure and properties
â”‚   Usage: python data_explorer.py
â”‚   Output: JSON report, image list CSV
â”‚   Best for: Understanding your dataset
â”‚
â”œâ”€â”€ vit.ipynb (existing)
â”‚   Purpose: Vision Transformer experiments
â”‚   Note: Part of original project
â”‚
â”œâ”€â”€ vit_ordered.ipynb (existing)
â”‚   Purpose: Ordered Vision Transformer implementation
â”‚   Note: Part of original project
â”‚
â””â”€â”€ vit_simple.py (existing)
    Purpose: Simple Vision Transformer baseline
    Note: Part of original project
```

### Annotations & Output (`annotations/`)
```
annotations/
â”‚
â”œâ”€â”€ CAPTION_VALIDATION_CHECKLIST.md â­â­
â”‚   â””â”€ Quality criteria, phase-specific validation, examples
â”‚
â”œâ”€â”€ DATASET_TEMPLATE.json
â”‚   â””â”€ JSON template for final annotated dataset
â”‚
â”œâ”€â”€ captions.json (generated)
â”‚   â””â”€ Raw captions from LLM
â”‚
â”œâ”€â”€ captions_validated.json (generated)
â”‚   â””â”€ Filtered and validated captions
â”‚
â”œâ”€â”€ annotated_dataset.json (generated)
â”‚   â””â”€ Complete dataset with all metadata
â”‚
â”œâ”€â”€ annotated_dataset.csv (generated)
â”‚   â””â”€ CSV version for Excel/spreadsheets
â”‚
â”œâ”€â”€ quality_metrics.json (generated)
â”‚   â””â”€ Statistical analysis of captions
â”‚
â”œâ”€â”€ validation_report.json (generated)
â”‚   â””â”€ Results of caption validation
â”‚
â”œâ”€â”€ dataset_exploration_report.json (generated)
â”‚   â””â”€ Analysis of dataset structure
â”‚
â”œâ”€â”€ image_list.csv (generated)
â”‚   â””â”€ All images with metadata
â”‚
â”œâ”€â”€ dataset_distribution.png (generated)
â”‚   â””â”€ Bar chart of images per phase
â”‚
â””â”€â”€ sample_images_with_captions.png (generated)
    â””â”€ Visual preview of sample images
```

### Dataset Structure (`balanced_crystallization/`)
```
balanced_crystallization/
â”‚
â”œâ”€â”€ phy_sugar_db/
â”‚   â”œâ”€â”€ unsaturated/      (Images)
â”‚   â”œâ”€â”€ labile/           (Images)
â”‚   â”œâ”€â”€ intermediate/     (Images)
â”‚   â””â”€â”€ metastable/       (Images)
â”‚
â”œâ”€â”€ phy_sugar_opr/
â”‚   â”œâ”€â”€ unsaturated/      (Images)
â”‚   â”œâ”€â”€ labile/           (Images)
â”‚   â”œâ”€â”€ intermediate/     (Images)
â”‚   â””â”€â”€ metastable/       (Images)
â”‚
â””â”€â”€ vir_polymer/
    â”œâ”€â”€ unsaturated/      (Images)
    â”œâ”€â”€ labile/           (Images)
    â”œâ”€â”€ intermediate/     (Images)
    â””â”€â”€ metastable/       (Images)
```

---

## ğŸ“š Reading Guide by Use Case

### Use Case 1: "I just want to get started quickly"
1. Read: [QUICK_START.md](QUICK_START.md) (5 min)
2. Run: `python LLM/data_explorer.py` (1 min)
3. Open: `LLM/captioning_interactive.ipynb` (30+ min)
4. Follow notebook cells sequentially

### Use Case 2: "I want to understand the project deeply"
1. Read: [README.md](README.md) (10 min)
2. Read: [PROJECT_GUIDE.md](PROJECT_GUIDE.md) (20 min)
3. Study: Phase definitions and examples
4. Review: [CAPTION_VALIDATION_CHECKLIST.md](annotations/CAPTION_VALIDATION_CHECKLIST.md)
5. Explore: Dataset with `data_explorer.py`

### Use Case 3: "I want to generate captions for my data"
1. Organize images by phase in subdirectories
2. Run: `python LLM/data_explorer.py` to analyze
3. Set up API key (see QUICK_START.md)
4. Use: Either `captioning_pipeline.py` or `captioning_interactive.ipynb`
5. Validate: Using `CAPTION_VALIDATION_CHECKLIST.md`
6. Export: As JSON/CSV for training

### Use Case 4: "I want to validate and filter captions"
1. Read: [CAPTION_VALIDATION_CHECKLIST.md](annotations/CAPTION_VALIDATION_CHECKLIST.md)
2. Use: Quality scoring rubric (90-100 = Excellent)
3. Manually review 10-20% of captions
4. Document issues and corrections
5. Regenerate poor captions
6. Create `captions_validated.json`

### Use Case 5: "I want to train a model with the captions"
1. Complete caption generation and validation
2. Load: `annotated_dataset.json`
3. Reference: Integration section in [PROJECT_GUIDE.md](PROJECT_GUIDE.md)
4. Create: PyTorch Dataset class
5. Train: Vision Transformer or other model
6. See: `LLM/vit.ipynb` for examples

---

## ğŸ”‘ Key Concepts

### Files by Purpose

#### ğŸ“– Learning/Understanding
| File | Content | Time |
|------|---------|------|
| README.md | Overview & context | 10 min |
| PROJECT_GUIDE.md | Full methodology | 30 min |
| QUICK_START.md | Get started fast | 5 min |
| CAPTION_VALIDATION_CHECKLIST.md | Quality criteria | 15 min |

#### ğŸ› ï¸ Tools/Scripts
| File | Purpose | Complexity |
|------|---------|-----------|
| data_explorer.py | Dataset analysis | â­ Easy |
| captioning_pipeline.py | Batch processing | â­â­ Medium |
| captioning_interactive.ipynb | Step-by-step workflow | â­â­ Medium |

#### ğŸ“Š Output/Data
| File | Type | When Generated |
|------|------|----------------|
| captions.json | Raw LLM output | After caption generation |
| annotated_dataset.json | Final dataset | After validation |
| quality_metrics.json | Statistics | Auto-generated |
| image_list.csv | Metadata | From data_explorer.py |

---

## ğŸš€ Typical Workflow

```
Start Here (Pick One)
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QUICK_START.md (5 min)              â”‚
â”‚ for: Just want to run it            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PROJECT_GUIDE.md (30 min)           â”‚
â”‚ for: Understanding everything       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Explore Dataset             â”‚
â”‚ Run: python LLM/data_explorer.py    â”‚
â”‚ Generates: report.json, image_list  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: Configure LLM               â”‚
â”‚ Set API key (see QUICK_START.md)    â”‚
â”‚ Choose: OpenAI or Anthropic         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Generate Captions           â”‚
â”‚ Method A: Jupyter notebook (learn)  â”‚
â”‚ Method B: CLI script (automate)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: Validate Quality            â”‚
â”‚ Use: CAPTION_VALIDATION_CHECKLIST   â”‚
â”‚ Generates: captions_validated.json  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 5: Use for Training            â”‚
â”‚ Load: annotated_dataset.json        â”‚
â”‚ Train: Vision Transformer models    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ Tips & Recommendations

### Before Running Scripts
- [ ] Read QUICK_START.md first
- [ ] Test with small sample (5-10 images)
- [ ] Monitor API costs carefully
- [ ] Keep API keys in environment variables
- [ ] Have annotations/ directory created

### During Execution
- [ ] Start with `data_explorer.py` to understand data
- [ ] Use Jupyter notebook first time for learning
- [ ] Test with single image before full batch
- [ ] Monitor API rate limits
- [ ] Save progress frequently

### After Caption Generation
- [ ] Review samples manually
- [ ] Check quality_metrics.json
- [ ] Use CAPTION_VALIDATION_CHECKLIST.md
- [ ] Validate at least 10% of captions
- [ ] Fix obvious errors
- [ ] Save validated version

---

## â“ Quick Answers

**Q: Where do I start?**
A: Read QUICK_START.md, then run data_explorer.py

**Q: How do I generate captions?**
A: Use either captioning_interactive.ipynb (learn) or captioning_pipeline.py (automate)

**Q: What's the output?**
A: JSON files with captions, CSV for Excel, PNG visualizations, quality metrics

**Q: How do I validate captions?**
A: Use CAPTION_VALIDATION_CHECKLIST.md and quality_score in JSON

**Q: How do I use captions for training?**
A: Load annotated_dataset.json, create PyTorch Dataset, see PROJECT_GUIDE.md

**Q: What if I encounter errors?**
A: See QUICK_START.md troubleshooting section

**Q: How much does it cost?**
A: ~$0.02-0.10 per image with LLM APIs (see QUICK_START.md)

---

## ğŸ”— Internal Links

### Main Documentation
- [README.md](README.md) - Project overview
- [PROJECT_GUIDE.md](PROJECT_GUIDE.md) - Detailed methodology
- [QUICK_START.md](QUICK_START.md) - Setup & execution

### Tools & Scripts
- [LLM/captioning_interactive.ipynb](LLM/captioning_interactive.ipynb) - Interactive workflow
- [LLM/captioning_pipeline.py](LLM/captioning_pipeline.py) - Automated pipeline
- [LLM/data_explorer.py](LLM/data_explorer.py) - Dataset analysis

### Validation & Reference
- [annotations/CAPTION_VALIDATION_CHECKLIST.md](annotations/CAPTION_VALIDATION_CHECKLIST.md) - Quality criteria
- [annotations/DATASET_TEMPLATE.json](annotations/DATASET_TEMPLATE.json) - Output structure

---

## ğŸ“ˆ Project Phases

### Phase 1: Planning & Setup âœ“
- Understand crystallization phases
- Prepare documentation
- Set up file structure
- Create scripts

### Phase 2: Exploration
- Run data_explorer.py
- Analyze dataset structure
- Understand image properties

### Phase 3: Caption Generation
- Configure LLM API
- Generate captions using notebook or CLI
- Monitor quality and costs

### Phase 4: Validation
- Review captions manually
- Calculate quality scores
- Identify and fix errors

### Phase 5: Finalization
- Create final dataset
- Export JSON/CSV
- Document changes
- Ready for training

### Phase 6: Training (Future)
- Use annotated_dataset.json
- Train Vision Transformer
- Evaluate performance

---

## ğŸ“ Support & Troubleshooting

### Common Issues
- **API Key Error** â†’ See QUICK_START.md section 2
- **Image Not Found** â†’ Check dataset path and structure
- **Rate Limit** â†’ Reduce batch size or wait
- **Poor Captions** â†’ Regenerate with better prompt

### Resources
- QUICK_START.md - Troubleshooting section
- PROJECT_GUIDE.md - Section 6 Next Steps
- CAPTION_VALIDATION_CHECKLIST.md - Issue examples

### Further Help
- Check error messages carefully
- Read full documentation
- Test with sample first
- Review example captions
- Refer to research paper

---

## ğŸ“‹ Checklist: Using This Guide

- [ ] Read README.md
- [ ] Read QUICK_START.md  
- [ ] Understand the 4 phases
- [ ] Run data_explorer.py
- [ ] Set up API key
- [ ] Generate first batch of captions
- [ ] Review captions against CHECKLIST
- [ ] Validate quality scores
- [ ] Export final dataset
- [ ] Prepare for model training

---

**Document Version:** 1.0  
**Last Updated:** 2025-12-18  
**Status:** Complete & Ready to Use

---

## Navigation Quick Links

ğŸ  [Home](README.md) â€¢ 
ğŸš€ [Quick Start](QUICK_START.md) â€¢ 
ğŸ“– [Full Guide](PROJECT_GUIDE.md) â€¢ 
âœ… [Validation](annotations/CAPTION_VALIDATION_CHECKLIST.md) â€¢ 
ğŸ› ï¸ [Tools](LLM/) â€¢ 
ğŸ“Š [Template](annotations/DATASET_TEMPLATE.json)
