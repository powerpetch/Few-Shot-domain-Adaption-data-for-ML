# Sugar Crystallization Image Captioning Project

## ğŸ¯ Project Goal

Transform the seed crystallization dataset from **non-captioning labels** (simple class names: Unsaturated, Labile, Intermediate, Metastable) into a **richly annotated dataset** with detailed captions explaining each crystallization phase.

**Why?** Captions provide:
- Detailed process descriptions for each phase
- Quantifiable metrics (growth percentages)
- Machine learning training data for vision-caption models
- Better understanding of phase transitions
- Cross-verification with multi-modal LLMs

---

## ğŸ“š Reference

**Research Foundation:**  
*"High-Intensified Resemblance and Statistic-Restructured Alignment in Few-Shot Domain Adaptation for Industrial-Specialized Employment"*
- Published: IEEE Transactions on Consumer Electronics, August 2023
- Authors: Jirayu Petchhan, Shun-Feng Su
- Focus: Few-shot domain adaptation techniques applicable to specialized industrial tasks

**Crystallization Theory:**  
Based on solubility diagrams showing:
1. **Unsaturated Zone**: No crystallization (below equilibrium line)
2. **Labile/Nucleation Zone**: Seed formation begins
3. **Meta-stable Zone**: Controlled crystal growth
4. **Stable Solution**: Equilibrium reached

---

## ğŸ—ï¸ Project Structure

```
d:\user\CEIPP\
â”œâ”€â”€ PROJECT_GUIDE.md                          # Complete project documentation
â”œâ”€â”€ QUICK_START.md                            # Get started in 3 steps
â”œâ”€â”€ 
â”œâ”€â”€ balanced_crystallization/
â”‚   â”œâ”€â”€ phy_sugar_db/
â”‚   â”‚   â”œâ”€â”€ unsaturated/     âœ“ Images
â”‚   â”‚   â”œâ”€â”€ labile/          âœ“ Images
â”‚   â”‚   â”œâ”€â”€ intermediate/    âœ“ Images
â”‚   â”‚   â””â”€â”€ metastable/      âœ“ Images
â”‚   â”œâ”€â”€ phy_sugar_opr/
â”‚   â”‚   â”œâ”€â”€ unsaturated/
â”‚   â”‚   â”œâ”€â”€ labile/
â”‚   â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â””â”€â”€ metastable/
â”‚   â””â”€â”€ vir_polymer/
â”‚       â”œâ”€â”€ unsaturated/
â”‚       â”œâ”€â”€ labile/
â”‚       â”œâ”€â”€ intermediate/
â”‚       â””â”€â”€ metastable/
â”‚
â”œâ”€â”€ annotations/                              # Output directory
â”‚   â”œâ”€â”€ CAPTION_VALIDATION_CHECKLIST.md      # Quality criteria
â”‚   â”œâ”€â”€ captions.json                        # Generated captions (LLM output)
â”‚   â”œâ”€â”€ captions_validated.json              # Validated captions (filtered)
â”‚   â”œâ”€â”€ annotated_dataset.json               # Final dataset with captions
â”‚   â”œâ”€â”€ annotated_dataset.csv                # CSV format for Excel
â”‚   â”œâ”€â”€ quality_metrics.json                 # Quality statistics
â”‚   â”œâ”€â”€ validation_report.json               # Validation results
â”‚   â”œâ”€â”€ dataset_exploration_report.json      # Dataset analysis
â”‚   â”œâ”€â”€ image_list.csv                       # All image metadata
â”‚   â”œâ”€â”€ dataset_distribution.png             # Chart of image counts
â”‚   â””â”€â”€ sample_images_with_captions.png      # Visual samples
â”‚
â””â”€â”€ LLM/                                      # Python scripts & notebooks
    â”œâ”€â”€ captioning_pipeline.py               # Command-line tool
    â”œâ”€â”€ captioning_interactive.ipynb         # Interactive notebook
    â”œâ”€â”€ data_explorer.py                     # Dataset exploration
    â”œâ”€â”€ vit.ipynb                            # Vision Transformer (existing)
    â”œâ”€â”€ vit_ordered.ipynb                    # Vision Transformer (existing)
    â””â”€â”€ vit_simple.py                        # Vision Transformer (existing)
```

---

## ğŸš€ Quick Start (3 Steps)

### Step 1: Install & Configure
```bash
# Navigate to project directory
cd d:\user\CEIPP

# Install dependencies
pip install openai anthropic pillow opencv-python pandas matplotlib

# Set API key (choose one)
$env:OPENAI_API_KEY="your_key_here"           # Windows PowerShell
set OPENAI_API_KEY=your_key_here               # Windows Command Prompt
export OPENAI_API_KEY="your_key_here"          # Linux/Mac
```

### Step 2: Run Exploration
```bash
python LLM/data_explorer.py
```
This will analyze your dataset and save reports to `annotations/`

### Step 3: Generate Captions

**Option A: Interactive (Recommended)**
```bash
cd LLM
jupyter notebook captioning_interactive.ipynb
```

**Option B: Command-line**
```bash
python LLM/captioning_pipeline.py \
  --input_dir balanced_crystallization/phy_sugar_db \
  --output_dir annotations \
  --provider openai
```

---

## ğŸ“– Documentation Files

| File | Purpose | Best For |
|------|---------|----------|
| `PROJECT_GUIDE.md` | Complete methodology & context | Understanding the full project |
| `QUICK_START.md` | Get started fast | Running the pipeline |
| `CAPTION_VALIDATION_CHECKLIST.md` | Quality criteria | Validating captions |
| `captioning_pipeline.py` | Batch processing tool | Large-scale automation |
| `captioning_interactive.ipynb` | Interactive notebook | Step-by-step workflow |
| `data_explorer.py` | Dataset analysis | Understanding your data |

---

## ğŸ”„ Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Explore Dataset     â”‚  Run data_explorer.py to understand structure
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Configure LLM       â”‚  Set API key, choose provider (OpenAI/Anthropic)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Generate Captions   â”‚  Use interactive notebook or pipeline script
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Validate Quality    â”‚  Use checklist to verify caption accuracy
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Filter & Refine     â”‚  Remove errors, regenerate poor captions
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. Create Dataset      â”‚  Export JSON/CSV for training
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. Train Models        â”‚  Use captions to train Vision Transformer, etc.
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Crystallization Phases Explained

### ğŸ”¹ UNSATURATED (à¹„à¸¡à¹ˆà¸­à¸´à¹ˆà¸¡à¸•à¸±à¸§)
- **Visual:** Clear transparent liquid, no particles
- **Growth:** 0%
- **What's Happening:** Solution is under-saturated, no crystallization yet
- **Next Step:** Heat or add more sugar to reach supersaturation

### ğŸ”¹ LABILE (à¹„à¸¡à¹ˆà¹€à¸ªà¸–à¸µà¸¢à¸£)
- **Visual:** Very tiny crystal seeds barely visible, mostly clear
- **Growth:** 5-15%
- **What's Happening:** Nucleation boundary reached, seeds just forming
- **Next Step:** Continue cooling gently to avoid oversaturation

### ğŸ”¹ INTERMEDIATE (à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡à¸¥à¸´à¹€à¸šà¸´à¸¥)
- **Visual:** Visible crystals growing, increasing cloudiness
- **Growth:** 15-50%
- **What's Happening:** Controlled growth phase, multiple nuclei developing
- **Next Step:** Maintain cooling rate for consistent growth

### ğŸ”¹ METASTABLE (à¹€à¸ªà¸–à¸µà¸¢à¸£à¸„à¸‡à¹€à¸ªà¹‰à¸™)
- **Visual:** Large well-formed crystals, fully concentrated solution
- **Growth:** 50-100%
- **What's Happening:** Stable equilibrium, crystallization nearly complete
- **Next Step:** Ready for harvest

---

## ğŸ“ Example Captions

### UNSATURATED Example:
```
UNSATURATED: Clear, transparent sugar solution with no visible crystal particles. 
The liquid is under-saturated and in stable equilibrium. No crystallization has 
occurred. Growth: 0%. Stage: Initial, awaiting supersaturation.
```

### LABILE Example:
```
LABILE: Very small crystal nuclei beginning to form in the super-saturated solution. 
Fine particles are barely visible, indicating the nucleation boundary has been crossed. 
The solution remains mostly clear but shows initial turbidity. 
Growth: ~10%. Stage: Primary nucleation, seed formation initiated.
```

### INTERMEDIATE Example:
```
INTERMEDIATE: Visible crystal growth with multiple nuclei developing. 
The solution is progressively becoming cloudy/turbid as crystals expand. 
Particle size is increasing noticeably, ranging from microscopic to small visible 
crystals. Growth: ~35%. Stage: Active controlled growth, crystals establishing structure.
```

### METASTABLE Example:
```
METASTABLE: Well-developed crystals fully formed and in stable growth phase. 
The solution is significantly saturated with mature crystal formations throughout. 
Color intensity is pronounced, indicating high solute concentration. 
Crystals show defined structure and size distribution. 
Growth: ~85%. Stage: Final equilibration, harvest-ready crystallization.
```

---

## ğŸ› ï¸ Tools & Technologies

### LLM Providers
- **OpenAI GPT-4 Vision**: ~$0.02-0.05 per image, high quality
- **Anthropic Claude 3 Opus**: ~$0.05-0.10 per image, excellent reasoning
- **Open-source** (LLaVA, BLIP-2): Free, requires GPU

### Python Libraries
- `openai` / `anthropic`: LLM API access
- `pillow` / `opencv-python`: Image processing
- `pandas`: Data management
- `matplotlib`: Visualization
- `json`: Data storage

### Output Formats
- **JSON**: `captions.json`, `annotated_dataset.json` (structured data)
- **CSV**: `annotated_dataset.csv` (Excel-compatible)
- **PNG**: Visualizations and sample images

---

## ğŸ’° Cost Estimation

| Task | Provider | Est. Cost (100 imgs) | Est. Cost (1000 imgs) |
|------|----------|---------------------|----------------------|
| Caption Generation | OpenAI GPT-4 | $2-5 | $20-50 |
| Caption Generation | Anthropic Claude | $5-10 | $50-100 |
| Data Exploration | Data Explorer | Free | Free |
| Validation | Manual Review | Time-dependent | - |

**Money-Saving Tips:**
- Start with 10-20 sample images
- Use cheaper models for testing
- Process in batches to monitor costs
- Cache prompts if provider supports it

---

## âœ… Quality Assurance

### Validation Checklist
Every caption should:
- âœ“ Identify the crystallization phase
- âœ“ Describe observable features
- âœ“ Include growth percentage
- âœ“ Reference process stage
- âœ“ Use correct technical terminology
- âœ“ Be grammatically correct
- âœ“ Be 100-300 characters long

See `CAPTION_VALIDATION_CHECKLIST.md` for full criteria.

### Quality Scoring
- **Excellent (90-100%):** Ready to use
- **Good (80-89%):** Minor revisions
- **Acceptable (70-79%):** Should review
- **Poor (<70%):** Regenerate

---

## ğŸ”„ Integration with Training

### Using Captions for Vision Transformer
```python
from torchvision import transforms
from torch.utils.data import Dataset

class CrystallizationDataset(Dataset):
    def __init__(self, json_file, transform=None):
        with open(json_file) as f:
            self.data = json.load(f)
        self.transform = transform
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        image = Image.open(item['image_path'])
        caption = item['caption']
        
        if self.transform:
            image = self.transform(image)
        
        return image, caption, item['phase_label']

# Use in training:
dataset = CrystallizationDataset('annotated_dataset.json')
```

---

## ğŸ› Troubleshooting

### "API key not found"
- Verify environment variable is set
- Restart terminal after setting
- Check API key is valid on provider website

### "Image file not found"
- Verify dataset path is correct
- Check images are in phase subdirectories
- Verify image format (JPG, PNG, JPEG)

### "Rate limit exceeded"
- Reduce number of images processed
- Wait 1-2 minutes before retrying
- Check API usage dashboard

### "Poor caption quality"
- Adjust LLM system prompt
- Try different LLM provider
- Manually review and correct
- Regenerate using improved prompt

See `QUICK_START.md` for more troubleshooting tips.

---

## ğŸ“š References & Citations

### Research Papers Referenced
- Petchhan, J., & Su, S. F. (2023). High-Intensified Resemblance and Statistic-Restructured Alignment in Few-Shot Domain Adaptation for Industrial-Specialized Employment. *IEEE Transactions on Consumer Electronics*, 69(3), 353-365.

### Related Concepts
- Few-shot learning and domain adaptation
- Vision transformers and multi-modal learning
- Crystallization kinetics and solubility diagrams
- Image captioning and vision-language models

---

## ğŸ“‹ Checklist: Before Starting

- [ ] Python 3.8+ installed
- [ ] API key obtained (OpenAI or Anthropic)
- [ ] Dataset extracted and organized
- [ ] Dependencies installed (`pip install -r requirements.txt` if available)
- [ ] Output directory created (`annotations/`)
- [ ] Read `QUICK_START.md`
- [ ] Understood the 4 crystallization phases

---

## ğŸ“ Learning Resources

### For Understanding Crystallization
- Study the `PROJECT_GUIDE.md` section on phases
- Review the solubility diagram in the reference PDF
- Look at examples in `CAPTION_VALIDATION_CHECKLIST.md`

### For Using the Tools
- See `QUICK_START.md` for 3-step setup
- Run `data_explorer.py` to understand your data
- Try the Jupyter notebook for interactive learning

### For Model Training
- Check `LLM/vit.ipynb` for Vision Transformer examples
- See integration section above
- Experiment with different architectures

---

## ğŸ“ Support & Help

### Common Questions

**Q: Can I use different datasets?**  
A: Yes! The pipeline works with any organized image dataset with subdirectories by class.

**Q: How do I use open-source LLMs locally?**  
A: Modify the scripts to use `transformers` library with LLaVA or BLIP-2.

**Q: Can I combine captions from multiple LLMs?**  
A: Yes! Generate with different providers and create an ensemble.

**Q: What if I run out of API budget?**  
A: Use manual captions, switch to open-source, or regenerate for the most important images only.

---

## ğŸ“„ License & Usage

This project is designed for research and educational purposes.

**Use responsibly:**
- Monitor API costs carefully
- Start with small samples before full runs
- Keep API keys secure (use environment variables)
- Cite the reference research paper if publishing

---

## ğŸš€ Next Steps

1. **Read** `QUICK_START.md` for immediate setup
2. **Explore** your data with `data_explorer.py`
3. **Generate** captions using `captioning_interactive.ipynb`
4. **Validate** using `CAPTION_VALIDATION_CHECKLIST.md`
5. **Train** your models with the annotated dataset

---

## ğŸ“Š Project Statistics

| Metric | Value |
|--------|-------|
| Crystallization Phases | 4 |
| Subdatasets | 3 (phy_sugar_db, phy_sugar_opr, vir_polymer) |
| Expected Images | 100+ per phase |
| Caption Length | 100-300 characters |
| Supported LLM Providers | 2+ (OpenAI, Anthropic, custom) |
| Output Formats | JSON, CSV, PNG |
| Estimated Processing Time | 30-60 sec per image |

---

**Version:** 1.0  
**Created:** 2025-12-18  
**Status:** Production Ready  
**Last Updated:** 2025-12-18

---

## ğŸ“¬ Quick Links

- [PROJECT_GUIDE.md](PROJECT_GUIDE.md) - Full documentation
- [QUICK_START.md](QUICK_START.md) - Get started in 3 steps
- [LLM/captioning_interactive.ipynb](LLM/captioning_interactive.ipynb) - Interactive notebook
- [annotations/CAPTION_VALIDATION_CHECKLIST.md](annotations/CAPTION_VALIDATION_CHECKLIST.md) - Quality criteria

**Happy Captioning! ğŸ‰**
